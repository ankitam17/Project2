{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "<div id=\"toc-wrapper\" style=\"\"><div id=\"toc\" style=\"max-height: 742px;\"><ol class=\"toc-item\"><li><a href=\"#1.-Data-Exploration-and-Cleaning\">1. Data Exploration and Cleaning</a><ol class=\"toc-item\"><li><a href=\"#1.1-Handling-NAs\">1.1 Handling NAs</a></li><li><a href=\"#1.2-Cleaning-Text-Fields\">1.2 Cleaning Text Fields</a></li><li><a href=\"#1.3-Checking-Uniques-and-Cleaning\">1.3 Checking Uniques and Cleaning</a></li><li><a href=\"#1.4-Fixing-the-Datatypes\">1.4 Fixing the Datatypes</a></li><li><a href=\"#1.5-Analyzing-number_of_products-and-creating-a-SKU-master-data\">1.5 Analyzing number_of_products and creating a SKU master data</a></li><li><a href=\"#1.6-Analyzing-Customer-and-the-Demographics\">1.6 Analyzing Customer and the Demographics</a></li><li><a href=\"#1.7-Country-vs-City\">1.7 Country vs City</a></li><li><a href=\"#1.8-Payments\">1.8 Payments</a></li></ol></li><li><a href=\"#2.-Feature-Engineering\">2. Feature Engineering</a></li><li><a href=\"#3.-Visualizations\">3. Visualizations</a><ol class=\"toc-item\"><li><a href=\"#3.1-Revenue\">3.1 Revenue</a></li><li><a href=\"#3.2-Basket-Size\">3.2 Basket Size</a></li><li><a href=\"#3.3-Products\">3.3 Products</a></li><li><a href=\"#3.4-Frequencies---is_first_order,-country,-payment_type-and-user_gender\">3.4 Frequencies - is_first_order, country, payment_type and user_gender</a></li><li><a href=\"#3.5-Coupons\">3.5 Coupons</a></li><li><a href=\"#3.6-Number-of-Hits-on-Website\">3.6 Number of Hits on Website</a></li><li><a href=\"#3.7-How-is-the-Revenue-affected-by-Days\">3.7 How is the Revenue affected by Days</a></li><li><a href=\"#3.8-Multi-Purchasers\">3.8 Multi Purchasers</a></li><li><a href=\"#3.9-Geographic-Importance\">3.9 Geographic Importance</a></li></ol></li><li><a href=\"#4.-Product-Affinity\">4. Product Affinity</a></li><li><a href=\"#5.-Customer-Segmentation\">5. Customer Segmentation</a></li></ol></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting folium\n",
      "  Downloading folium-0.19.1-py2.py3-none-any.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/kitamathur/opt/anaconda3/lib/python3.9/site-packages (from folium) (1.20.3)\n",
      "Requirement already satisfied: jinja2>=2.9 in /Users/kitamathur/opt/anaconda3/lib/python3.9/site-packages (from folium) (2.11.3)\n",
      "Collecting branca>=0.6.0\n",
      "  Downloading branca-0.8.0-py3-none-any.whl (25 kB)\n",
      "Collecting xyzservices\n",
      "  Downloading xyzservices-2024.9.0-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 9.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/kitamathur/opt/anaconda3/lib/python3.9/site-packages (from folium) (2.26.0)\n",
      "Collecting jinja2>=2.9\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 40.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_10_9_universal2.whl (14 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kitamathur/opt/anaconda3/lib/python3.9/site-packages (from requests->folium) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kitamathur/opt/anaconda3/lib/python3.9/site-packages (from requests->folium) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kitamathur/opt/anaconda3/lib/python3.9/site-packages (from requests->folium) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/kitamathur/opt/anaconda3/lib/python3.9/site-packages (from requests->folium) (2.0.4)\n",
      "Installing collected packages: MarkupSafe, jinja2, xyzservices, branca, folium\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n",
      "cookiecutter 1.7.2 requires Jinja2<3.0.0, but you have jinja2 3.1.4 which is incompatible.\n",
      "cookiecutter 1.7.2 requires MarkupSafe<2.0.0, but you have markupsafe 3.0.2 which is incompatible.\u001b[0m\n",
      "Successfully installed MarkupSafe-3.0.2 branca-0.8.0 folium-0.19.1 jinja2-3.1.4 xyzservices-2024.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_8/33t6x7jj04x73mh8761v7k2c0000gn/T/ipykernel_17347/732364783.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the original excel, rename the columns and save it in csv\n",
    "df = pd.read_csv('MIQ_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df top 5 rows\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Handling NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape :: {df.shape}')\n",
    "\n",
    "print('NA :: ')\n",
    "print(df.isna().sum()/df.shape[0]*100) # get percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the given data we can see that the **birthday** is missing to a varying extent. There can be some cool imputation model that can be built based on the type of product bought. Given the time constraint, it is currently taken out of scope. So, we drop it.\n",
    "\n",
    "* We drop the column **country province** as of now.\n",
    "\n",
    "* We will impute the **coupons** with 'na' because it can unearth information that if a particular product is sku is only of promo type.\n",
    "\n",
    "* We drop the rows where **gender** and the **city** are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define drop columns\n",
    "to_drop_cols = ['user_birthday', 'country_province']\n",
    "to_drop_rows = ['user_gender', 'city']\n",
    "to_fillna = {'order_coupon_code': 'na'}\n",
    "\n",
    "# drop the columns/rows as discussed\n",
    "df = df.drop(to_drop_cols, axis=1)\n",
    "df = df.dropna(subset=to_drop_rows)\n",
    "df = df.fillna(to_fillna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the NA removal, we have lost $(49999-48491)/49999*100 = 3.02$% of the data which is acceptable.\n",
    "\n",
    "### 1.2 Cleaning Text Fields\n",
    "\n",
    "Let's do a quick check on the city and the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique city name with original casing and lower casing\n",
    "df.city.nunique(), df.city.str.lower().nunique(), df.city.str.strip().str.lower().nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there's discrepancy in the city names wrt casing. So, we lower and strip all the fields which can have text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change casing and trim extra spaces\n",
    "def text_preprocess(s):\n",
    "    return s.str.strip().str.lower()\n",
    "\n",
    "# normalize all the text columns\n",
    "for c in df.select_dtypes('object').columns:\n",
    "    df[c] = text_preprocess(df[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Checking Uniques and Cleaning\n",
    "\n",
    "Let's see some of the unique value counts and see if they make some sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we discuss any further, let's check the unique values of and also the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values in the below columns\n",
    "check_unq_val_cols = ['is_first_order', 'country', 'payment_type', 'user_gender']\n",
    "\n",
    "for i in check_unq_val_cols:\n",
    "    print(f'Checking {i} :: {df[i].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we unearth the hidden **undefined** values present in the data. We need to get rid of them for the future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace undefined with nan and then drop them\n",
    "df = df.replace('undefined', np.nan)\n",
    "\n",
    "print('NA :: ')\n",
    "print(df.isna().sum()/df.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we find only 0.3% of the data has noise. Let's get rid of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Number of products as 258** is also a bit confusing as it is highly unlikely that a person is shopping that many.\n",
    "* Since the **product name** and the **product IDs** have different unique count, then we can infer that there is **no 1:1 mapping between them**, which is a bit confusing.\n",
    "* Since we have lesser ids than the timestamp, we can infer that we have **repeat-customers**.\n",
    "* From the above uniques, we can see that the number of IPs and user IDs are different. Although it can be the case because no IPs are static nowadays, we can just check and in case find the IPs are varying for the same customer, we **can get rid of IP** too.\n",
    "\n",
    "### 1.4 Fixing the Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider changing the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the datatypes to the required format\n",
    "df['timestamp'] = pd.to_datetime(df.timestamp)\n",
    "df['is_first_order'] = df.is_first_order.astype('int')\n",
    "df['revenue'] = df.revenue.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Analyzing number_of_products and creating a SKU master data\n",
    "\n",
    "Mostly all except number of products could not be casted to integer because of *invalid literal for int() with base 10: '1,1'*. This makes us rethink the level at which the data is present and decide on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['timestamp', 'user_id', 'ip_address', 'user_gender','city', 'country', 'product_id', 'number_of_products']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first thing to note here is row 6 has 2 transactions (NI537AA69RPY and NI537AA97PBM). So, essentially **each transaction shows a basket** which we may keep like this or melt it. Let's see how different are the number_of_products are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.number_of_products.unique()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.number_of_products == '1,2,1'][['timestamp', 'user_id', 'ip_address', 'user_gender','city', 'country', 'product_id', 'number_of_products']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists from the text of product id, names and number bought\n",
    "df['product_id'] = df.product_id.apply(lambda x: x.split(','))\n",
    "df['product_name'] = df.product_name.apply(lambda x: x.split(','))\n",
    "df['number_of_products'] = df.number_of_products.apply(lambda x: list(map(int, x.split(','))))\n",
    "\n",
    "# do some basic feature engineering\n",
    "# create columns such as id count, names count and quantity count\n",
    "df['product_id_count'] = df.product_id.apply(lambda x: len(x))\n",
    "df['product_name_count'] = df.product_name.apply(lambda x: len(x))\n",
    "df['total_products'] = df.number_of_products.apply(lambda x: sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.product_id_count != df.product_name_count][['product_id', 'product_id_count', 'product_name', 'product_name_count']].shape)\n",
    "df[df.product_id_count != df.product_name_count][['product_id', 'product_id_count', 'product_name', 'product_name_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can infer that around 264 transactions have noise and the number of product names does not map with the number of product ids. For cleaning the data, we need to get rid of them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid if the transactions were the counts of id and names are not same\n",
    "df = df[df.product_id_count == df.product_name_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we ought to check if there is a 1:1 mapping between the product ids and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a flat list of the product names and ids\n",
    "p_id = [j for i in df.product_id for j in i]\n",
    "p_name = [j for i in df.product_name for j in i]\n",
    "\n",
    "# create a df with the id and names\n",
    "sku_master = pd.DataFrame(columns = ['id', 'name'])\n",
    "sku_master['id'] = p_id\n",
    "sku_master['name'] = p_name\n",
    "sku_master = sku_master.drop_duplicates()\n",
    "\n",
    "# find the ids which has more than 1 name\n",
    "t = sku_master.groupby('id').count()\n",
    "print(f'{t[t.name>1].shape[0]/t.shape[0]*100:.2f}% of product ids have more than 1 description')\n",
    "print(t[t.name>1].head())\n",
    "\n",
    "# normalize the ids with more than one name with the name haaving maximum length\n",
    "sku_master = pd.DataFrame(sku_master.groupby('id').name.apply(list))\n",
    "sku_master = sku_master.name.apply(lambda x: max(x, key=len))\n",
    "\n",
    "# reset the product names in the original df\n",
    "df['product_name'] = [[sku_master[j] for j in i] for i in df.product_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalized all the product names by taking the longest possible available description for the same item.\n",
    "With the removals, we have lost $(49999−48214)/49999∗100=3.57$ % of the data till now.\n",
    "\n",
    "\n",
    "\n",
    "So, in these examples, we can confirm that number of products actually shows the quantity.\n",
    "\n",
    "However, in the initial head, there seems to be one more observation. `The user_id==0` is making purchases at a gap of 2 minutes from different countries and also has different gender.\n",
    "\n",
    "This makes us question the following things.\n",
    "* Is the user_id consistent with the demographics?\n",
    "* Is the country/city is based on IP?\n",
    "\n",
    "### 1.6 Analyzing Customer and the Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_cols = ['user_gender', 'country']\n",
    "t = df.groupby('user_id').agg(dict(zip(demographics_cols, ['nunique']*len(demographics_cols))))\n",
    "\n",
    "# get metrics where user has more than 1 gender and country\n",
    "print(f'Unique Customers :: {t.shape[0]}')\n",
    "print(f'Customers who have more than 1 demographics info:: {[(c, t[t[c]>1].shape[0]) for c in demographics_cols]}')\n",
    "print(f'Transaction which comprises of these users :: {df[df.user_id.isin(list(set([j for i in [t[t[c]>1].index for c in demographics_cols] for j in i])))].shape[0]/df.shape[0]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is only around 1.2% of the transactions, we get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.user_id.isin(list(set([j for i in [t[t[c]>1].index for c in demographics_cols] for j in i])))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Country vs City\n",
    "How can a city have 2 countries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get count of country for each city\n",
    "t = df.groupby('city').agg({'country': 'nunique'})\n",
    "t = t[t.country>1]\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now definitely this can be true, that is a place with same name co-exist in two countries. But we have been checking on Google Maps and some of them exist only in a Australia and not New Zealand.\n",
    "\n",
    "What we come up with is, normalizing by mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the country which has the highest occurence for a city\n",
    "def normalize_country(city):\n",
    "    country = df[df.city == city]['country'].value_counts().index[0]\n",
    "    df.loc[df.city == city, 'country'] = country\n",
    "    \n",
    "# normalize the counties\n",
    "for city in t.index:\n",
    "    normalize_country(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Payments\n",
    "\n",
    "There is something called 'no-payment'. What does it mean? Let's check the revenue earned from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.payment_type == 'nopayment']['revenue'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these transactions contributed nothing to revenue. Let's drop them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cases where there is nopayment\n",
    "df = df[df.payment_type != 'nopayment']\n",
    "df = df.drop(['ip_address', 'product_name_count'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have lost $(49999−46905)/49999∗100=6.2$ % data because of all the treatments.\n",
    "\n",
    "Hopefully, the data is clean now. Also, we already identified some of the columns for whom we can see the distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some new columns from timestamp\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['date'] = df.timestamp.apply(lambda x: str(x).split()[0])\n",
    "df['hour'] = df.timestamp.apply(lambda x: str(x).split()[1].split(':')[0])\n",
    "df['weekday'] = df.timestamp.apply(lambda x: x.weekday()) # Monday is 0, Sunday == 6\n",
    "df['week_no'] = df.timestamp.apply(lambda x: x.week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations\n",
    "\n",
    "### 3.1 Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = [j for i in fig.subplots(2,2) for j in i]\n",
    "\n",
    "df.revenue.plot(kind='hist', color='gray', ax=ax[0]);\n",
    "df.revenue.plot(kind='box', color='gray', ax=ax[1]);\n",
    "df[df.revenue<200].revenue.plot(kind='hist', color='gray', ax=ax[2]);\n",
    "df[df.revenue<200].revenue.plot(kind='box', color='gray', ax=ax[3]);\n",
    "\n",
    "ax[0].set_xlabel('revenue')\n",
    "ax[0].set_title('Frequency Distribution of Revenue')\n",
    "\n",
    "ax[1].set_title('Box Plot of Revenue')\n",
    "\n",
    "ax[2].set_xlabel('revenue')\n",
    "ax[2].set_title('Frequency Distribution of Revenue (after clipping outliers)')\n",
    "\n",
    "ax[3].set_title('Box Plot of Revenue (after clipping outliers)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives rise to the question - Is the revenue related with the number of products bought (basket size)?\n",
    "\n",
    "### 3.2 Basket Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,4))\n",
    "ax = fig.subplots()\n",
    "\n",
    "df.total_products.value_counts().plot(kind='bar', ax=ax, color='gray')\n",
    "ax.set_title('Distribution of Basket Size')\n",
    "ax.set_ylabel('count')\n",
    "ax.set_xlabel('basket size');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "ax = [i for i in fig.subplots(1,2)]\n",
    "\n",
    "sns.scatterplot(x=df.total_products, y=df.revenue, ax=ax[0], color='gray')\n",
    "sns.violinplot(x=df.total_products, y=df.revenue, ax=ax[1])\n",
    "\n",
    "ax[0].set_title('Basket Size vs Revenue - Scatter')\n",
    "ax[0].set_xlabel('basket size')\n",
    "\n",
    "ax[1].set_title('Basket Size vs Revenue - Violin')\n",
    "ax[1].set_xlabel('basket size');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('user_id').timestamp.count().value_counts().plot(kind='bar', color='gray', figsize=(7,4))\n",
    "# plt.title('Frequency of Purchases by Users')\n",
    "# plt.xlabel('purchase count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total Sales :: {df.revenue.sum():.2f}')\n",
    "print(f'Average Basket Size :: {np.mean(df.total_products):.2f}')\n",
    "print(f'Average Revenue per transaction :: {np.mean(df.revenue):.2f}')\n",
    "print(f'Average Revenue per quantity of Product :: {np.mean(df.revenue/df.total_products):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_name = [j for i in df.product_name for j in i]\n",
    "p_id = [j for i in df.product_id for j in i]\n",
    "p_count = [j for i in df.number_of_products for j in i]\n",
    "\n",
    "t = pd.DataFrame(columns = ['name', 'count'])\n",
    "t['name'] = p_name\n",
    "t['count'] = p_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of SKU master :: {sku_master.shape[0]}')\n",
    "print(f'Shape of SKU master unique SKU names :: {sku_master.drop_duplicates().shape[0]}')\n",
    "print(f'Shape of unique SKU sold by names :: {len(set(p_name))}')\n",
    "\n",
    "print(f'Shape of unique SKU sold by ids :: {len(set(p_id))}')\n",
    "print('It has to be noted that 2 products can have same name, but different ids.')\n",
    "\n",
    "print(f'Overall SKU sales :: {sum(p_count)} or {df.total_products.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.groupby('name').sum().sort_values('count', ascending=False)[:25].plot(kind='bar', color='gray', figsize=(7,4))\n",
    "\n",
    "plt.xlabel('Product Name')\n",
    "plt.title('Top 25 Products');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['count'].value_counts().plot(kind='bar', color='gray', figsize=(7,4))\n",
    "plt.title('Distribution of times a Product was bought')\n",
    "plt.xlabel('times a product was bought');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Frequencies - is_first_order, country, payment_type and user_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 8))\n",
    "ax = [j for i in fig.subplots(2,2) for j in i]\n",
    "\n",
    "for c, i in enumerate(check_unq_val_cols):\n",
    "    df[i].value_counts().plot(kind='bar', color='gray', ax=ax[c]);\n",
    "    ax[c].set_title(i)\n",
    "    ax[c].set_ylabel('count')\n",
    "    \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a number of repeat customers. Let's see the % of coupon used among the repeat customers and the new ones.\n",
    "\n",
    "### 3.5 Coupons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({'coupon': df[df.order_coupon_code != 'na'].shape[0], \n",
    "           'no_coupon': df[df.order_coupon_code == 'na'].shape[0]}).plot(kind='bar', color='gray', figsize=(7, 4))\n",
    "\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = df['is_first_order'].value_counts()\n",
    "t2 = df[df.order_coupon_code != 'na']['is_first_order'].value_counts()\n",
    "\n",
    "(t2/t1).plot(kind='bar', color='gray', figsize=(7, 4))\n",
    "plt.ylabel('percentage')\n",
    "plt.title('% of is_first_order when a coupon is applied');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = df['payment_type'].value_counts()\n",
    "t2 = df[df.order_coupon_code != 'na']['payment_type'].value_counts()\n",
    "\n",
    "(t2/t1).plot(kind='bar', color='gray', figsize=(7, 4))\n",
    "plt.ylabel('percentage')\n",
    "plt.title('% of payments when a coupon is applied');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since we have lesser ids than the timestamp, we can infer that we have **repeat-customers**.\n",
    "* From the above uniques, we can see that the number of IPs and user IDs are different. Although it can be the case because no IPs are static nowadays, we can just check and in case find the IPs are varying for the same customer, we **can get rid of IP** too.\n",
    "* Since the **product name** and the **product IDs** have different unique count, then we can infer that there is **no 1:1 mapping between them**, which is a bit confusing.\n",
    "* Number of products as 258 is also a bit confusing as it is highly unlikely that a person is shopping that many."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Number of Hits on Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby('weekday').agg({'timestamp': 'count'}).plot(figsize=(9, 5), color='gray')\n",
    "\n",
    "ax.set_xticks(range(7))\n",
    "ax.set_xticklabels(day_order)\n",
    "\n",
    "ax.set_ylabel('count')\n",
    "ax.set_title('Count of Transactions for Each Day');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby('weekday').agg({'revenue': 'sum'}).plot(figsize=(9, 5), color='gray')\n",
    "\n",
    "ax.set_xticks(range(7))\n",
    "ax.set_xticklabels(day_order)\n",
    "\n",
    "ax.set_ylabel('sum')\n",
    "ax.set_title('Sum of Transactions for Each Day');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby(['week_no', 'weekday']).agg({'timestamp': 'count'}).reset_index().sort_values(['week_no', 'weekday'])\n",
    "\n",
    "ax = t.timestamp.plot(figsize=(9, 5), color='gray')\n",
    "\n",
    "ax.set_xticks(range(0, 21))\n",
    "ax.set_xticklabels([dict(zip(range(7), day_order))[i] for i in t.weekday])\n",
    "\n",
    "ax.axvline(x=3, color='r', linestyle='--')\n",
    "ax.axvline(x=9, color='r', linestyle='--')\n",
    "ax.axvline(x=16, color='r', linestyle='--')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.set_ylabel('count')\n",
    "ax.set_title('Count of Transactions for Each Day');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby(['week_no', 'weekday']).agg({'revenue': 'sum'}).reset_index().sort_values(['week_no', 'weekday'])\n",
    "t = pd.DataFrame(list(itertools.product(range(39, 43), range(0, 7))), columns=['week_no', 'weekday']).merge(t, how='left')\n",
    "t = pd.DataFrame(dict(t.groupby('week_no')['revenue'].apply(list)))\n",
    "t.columns = [f'week_no_{i}' for i in t.columns]\n",
    "\n",
    "fig = plt.figure(figsize = (9, 5))\n",
    "ax = fig.add_subplot()\n",
    "# ax.plot(t.week_no_39, color = \"red\", alpha = .5, label='week_39')\n",
    "ax.plot(t.week_no_40, color = \"blue\", alpha = .5, label='week_40')\n",
    "ax.plot(t.week_no_41, color = \"green\", alpha = .5, label='week_41')\n",
    "ax.plot(t.week_no_42, color = \"black\", alpha = .5, label='week_42')\n",
    "\n",
    "ax.set_xticks(range(7))\n",
    "ax.set_xticklabels(day_order)\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "ax.set_ylabel('sum of revenue')\n",
    "ax.set_title('Sum of Revenue for Each Weekday across Weeks');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby(['hour']).agg({'revenue': 'sum'}).plot(figsize=(9, 5), color='gray')\n",
    "\n",
    "ax.set_xticks(range(0, 24))\n",
    "ax.set_xticklabels(['12 am'] + [f'{i} am' for i in range(1, 12)] + ['12 pm'] + [f'{i} pm' for i in range(1, 12)])\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.axvline(x=10.5, color='r', linestyle='--')\n",
    "ax.axvline(x=18.5, color='r', linestyle='--')\n",
    "\n",
    "ax.set_ylabel('sum of revenue')\n",
    "ax.set_title('Sum of Revenue for Each Hour');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby(['weekday', 'hour']).agg({'revenue': 'count'}).reset_index()\n",
    "t\n",
    "\n",
    "fig = plt.figure(figsize = (9, 5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.plot(t[t.weekday==2].revenue.values, color = \"green\", alpha = .5, label='Wednesday')\n",
    "ax.plot(t[t.weekday==5].revenue.values, color = \"blue\", alpha = .5, label='Saturday')\n",
    "ax.plot(t[t.weekday==6].revenue.values, color = \"black\", alpha = .5, label='Sunday')\n",
    "\n",
    "ax.set_ylim([0, 800])\n",
    "ax.set_xticks(range(0, 24))\n",
    "ax.set_xticklabels(['12 am'] + [f'{i} am' for i in range(1, 12)] + ['12 pm'] + [f'{i} pm' for i in range(1, 12)])\n",
    "\n",
    "ax.axvline(x=10.5, color='r', linestyle='--')\n",
    "ax.axvline(x=18.5, color='r', linestyle='--');\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "ax.set_ylabel('sum of revenue')\n",
    "ax.set_title('Sum of Revenue for Each Hour across days');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby(['week_no', 'hour']).agg({'revenue': 'count'}).reset_index()\n",
    "\n",
    "fig = plt.figure(figsize = (9, 5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# ax.plot(t[t.week_no==39].revenue.values, color = \"red\", alpha = .5, label='week_40')\n",
    "ax.plot(t[t.week_no==40].revenue.values, color = \"blue\", alpha = .5, label='week_40')\n",
    "ax.plot(t[t.week_no==41].revenue.values, color = \"green\", alpha = .5, label='week_41')\n",
    "ax.plot(t[t.week_no==42].revenue.values, color = \"black\", alpha = .5, label='week_42')\n",
    "\n",
    "ax.set_xticks(range(0, 24))\n",
    "ax.set_xticklabels(['12 am'] + [f'{i} am' for i in range(1, 12)] + ['12 pm'] + [f'{i} pm' for i in range(1, 12)])\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.axvline(x=10.5, color='r', linestyle='--')\n",
    "ax.axvline(x=18.5, color='r', linestyle='--')\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "ax.set_ylabel('sum of revenue')\n",
    "ax.set_title('Sum of Revenue for Each Hour across weeks');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 How is the Revenue affected by Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby(['week_no', 'weekday']).agg({'revenue': 'mean'}).reset_index().sort_values(['week_no', 'weekday'])\n",
    "\n",
    "ax = t.revenue.plot(figsize=(9, 5), color='gray')\n",
    "\n",
    "ax.set_xticks(range(0, 21))\n",
    "ax.set_xticklabels([dict(zip(range(7), day_order))[i] for i in t.weekday])\n",
    "\n",
    "ax.axvline(x=0, color='r', linestyle='--')\n",
    "ax.axvline(x=7, color='r', linestyle='--')\n",
    "ax.axvline(x=14, color='r', linestyle='--')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.set_ylabel('mean revenue')\n",
    "ax.set_title('Mean Revenue for Each Day');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby('weekday').agg({'revenue': 'mean'}).plot(figsize=(9, 5), color='gray')\n",
    "\n",
    "ax.set_xticks(range(7))\n",
    "ax.set_xticklabels(day_order)\n",
    "\n",
    "ax.set_ylabel('mean revenue')\n",
    "ax.set_title('Mean Revenue for Each Weekday');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby(['week_no', 'weekday']).agg({'revenue': 'mean'}).reset_index().sort_values(['week_no', 'weekday'])\n",
    "t = pd.DataFrame(list(itertools.product(range(39, 43), range(0, 7))), columns=['week_no', 'weekday']).merge(t, how='left')\n",
    "t = pd.DataFrame(dict(t.groupby('week_no')['revenue'].apply(list)))\n",
    "t.columns = [f'week_no_{i}' for i in t.columns]\n",
    "\n",
    "fig = plt.figure(figsize = (9, 5))\n",
    "ax = fig.add_subplot()\n",
    "# ax.plot(t.week_no_39, color = \"red\", alpha = .5, label='week_39')\n",
    "ax.plot(t.week_no_40, color = \"blue\", alpha = .5, label='week_40')\n",
    "ax.plot(t.week_no_41, color = \"green\", alpha = .5, label='week_41')\n",
    "ax.plot(t.week_no_42, color = \"black\", alpha = .5, label='week_42')\n",
    "\n",
    "ax.set_ylim([125, 148])\n",
    "ax.set_xticks(range(7))\n",
    "ax.set_xticklabels(day_order)\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "ax.set_ylabel('mean revenue')\n",
    "ax.set_title('Mean Revenue for Each Weekday across Weeks');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby(['payment_type']).agg({'revenue': 'mean'}).plot(kind='bar', color='gray', figsize=(7, 4))\n",
    "ax.set_ylim([0, 170])\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_ylabel('mean revenue')\n",
    "ax.set_title('Mean Revenue across different Payment Types');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Multi Purchasers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby('user_id').count()[['timestamp']]\n",
    "print(f'% of customers who are doing multi-purchase :: {t[t.timestamp>1].shape[0]/t.shape[0]*100:.2f}')\n",
    "print(f'% of customers who are doing single-purchase :: {t[t.timestamp==1].shape[0]/t.shape[0]*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = df[df.user_id.isin(t[t.timestamp>1].index)]\n",
    "sp = df[~df.user_id.isin(t[t.timestamp>1].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mp.hour.astype('int').plot(kind='hist', color='gray', figsize=(7, 4), bins=20)\n",
    "ax.set_xlabel('hours')\n",
    "ax.set_title('Histogram of hours at which Purchases are made');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mp.weekday.astype('int').value_counts().plot(kind='bar', color='gray', figsize=(7, 4))\n",
    "ax.set_xlabel('days')\n",
    "ax.set_title('Distribution of Purchases made across Days');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.Series({'multi-purchase': mp.revenue.mean(), 'single purchase': sp.revenue.mean()}).plot(kind='bar', color='gray', figsize=(7, 4))\n",
    "ax.set_ylim([130, 143])\n",
    "ax.set_ylabel('mean revenue')\n",
    "ax.set_title('Mean Revenue of Muti-Purchase vs Single-Purchase');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame([mp.payment_type.value_counts()/mp.shape[0], sp.payment_type.value_counts()/sp.shape[0]]).T\n",
    "t.columns = ['multi_purchasers', 'single_purchasers']\n",
    "ax = t.plot(kind='bar', color=['gray', 'black'], figsize=(7, 4), alpha=0.7)\n",
    "ax.set_ylim([0, 0.55])\n",
    "\n",
    "ax.set_ylabel('%purchase')\n",
    "ax.set_title('% Purchase made via different cards for Sigle and Multi Purchases');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = mp[['user_id']].join(mp[['user_id', 'timestamp']].groupby('user_id').diff())\n",
    "t.timestamp = pd.to_timedelta(t.timestamp).astype(np.int64, errors='ignore').dropna()\n",
    "t = t.dropna().groupby('user_id').timestamp.apply(list).values\n",
    "\n",
    "fig = plt.figure(figsize = (7, 4))\n",
    "ax = fig.add_subplot()\n",
    "plt.hist([i.days for i in list(map(np.mean, t))], color='gray', bins=20)\n",
    "\n",
    "ax.set_ylabel('count')\n",
    "ax.set_xlabel('mean frequency of purchase in days')\n",
    "ax.set_title('Distribution of Mean Frequency of Purchase (for each user) in days');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average Purchase Frequency: {np.mean([i.days for i in list(map(np.mean, t))]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Geographic Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = pd.read_csv('worldcities.csv')[['city', 'lat', 'lng', 'population']]\n",
    "city.city = city.city.str.lower()\n",
    "t = df.merge(city, on='city')\n",
    "t = t.groupby(['lat', 'lng']).agg({'revenue': 'mean', 'city': 'max'}).reset_index()\n",
    "\n",
    "m = folium.Map([-29.043995, 138.264296], zoom_start=4)\n",
    "for index, row in t.iterrows():\n",
    "    \n",
    "    if row['revenue'] >=300:\n",
    "        fill_color = 'darkred'\n",
    "    if row['revenue'] >=150:\n",
    "        fill_color = 'red'\n",
    "    if row['revenue'] >=100:\n",
    "        fill_color = 'orange'\n",
    "    else:\n",
    "        fill_color = 'darkpurple'\n",
    "    \n",
    "    folium.Circle(\n",
    "          location=[row['lat'], row['lng']],\n",
    "          tooltip=f\"{row['city']} | {row['revenue']}\",\n",
    "          radius=row['revenue']*400,\n",
    "          color='darkred',\n",
    "          fill=True,\n",
    "          fill_color=fill_color,\n",
    "    ).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = pd.read_csv('worldcities.csv')[['city', 'lat', 'lng', 'population']]\n",
    "city.city = city.city.str.lower()\n",
    "t = df.merge(city, on='city')\n",
    "t = t.groupby(['lat', 'lng']).agg({'revenue': 'sum', 'city': 'max'}).reset_index()\n",
    "\n",
    "m = folium.Map([-29.043995, 138.264296], zoom_start=4)\n",
    "for index, row in t.iterrows():\n",
    "    \n",
    "    if row['revenue'] >=5000:\n",
    "        row['revenue'] = 2000\n",
    "        fill_color = 'darkred'\n",
    "    if row['revenue'] >=150:\n",
    "        fill_color = 'red'\n",
    "    if row['revenue'] >=100:\n",
    "        fill_color = 'orange'\n",
    "    else:\n",
    "        fill_color = 'darkpurple'\n",
    "    \n",
    "    folium.Circle(\n",
    "          location=[row['lat'], row['lng']],\n",
    "          tooltip=f\"{row['city']} | {row['revenue']}\",\n",
    "          radius=row['revenue']*40,\n",
    "          color='darkred',\n",
    "          fill=True,\n",
    "          fill_color=fill_color,\n",
    "    ).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.city.value_counts()[:25].plot(kind='bar', color='gray', figsize=(10, 5))\n",
    "\n",
    "ax.set_ylabel('count')\n",
    "ax.set_xlabel('frequency of city')\n",
    "ax.set_title('Distribution of Frequency of City');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Product Affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 =  df.product_name.apply(lambda x: ' '.join(x)).str.cat(sep=' ')\n",
    "wordcloud_1 = WordCloud(background_color='white', collocations=False).generate(text_1)\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.imshow(wordcloud_1, interpolation = \"bilinear\")\n",
    "plt.title('Wordcloud for the Product Description', fontsize=15)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These is the wordcloud on the product description. We can see mostly female-related things are sold such as top, lace, sleeves, leather, heel etc. This justifies the gender imbalance in the dataset.\n",
    "\n",
    "Based on the data, we will run FP Growth to generate the frequent itemsets and then use it for the association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "dataset = df.product_id\n",
    "\n",
    "dataset = df[df.product_id_count>3].product_id\n",
    "\n",
    "te_ary = te.fit(dataset).transform(dataset, sparse=True)\n",
    "sparse_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = fpgrowth(sparse_df, min_support=0.0007, use_colnames=True, verbose=False)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets2 = frequent_itemsets\n",
    "\n",
    "rules = association_rules(frequent_itemsets2, metric=\"confidence\", min_threshold=0.6).\\\n",
    "        sort_values(['support', 'confidence'], ascending=False)\n",
    "rules['antecedents'] = [', '.join([sku_master[j] for j in i]) for i in rules.antecedents]\n",
    "rules['consequents'] = [', '.join([sku_master[j] for j in i]) for i in rules.consequents]\n",
    "rules = rules.drop_duplicates(['antecedents', 'consequents'])\n",
    "rules = rules[rules.antecedents!=rules.consequents][:25]\n",
    "[f\"{i[0]} → {i[1]}\" for i in zip(rules.antecedents, rules.consequents) if (i[0] not in i[1])and(i[1] not in i[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby('user_id').agg({\n",
    "    'is_first_order': 'max',\n",
    "    'user_gender': 'max',\n",
    "    'country': 'max',\n",
    "    'revenue': 'mean',\n",
    "    'total_products': 'max'\n",
    "})\n",
    "\n",
    "t = t.replace({'au': 0, 'nz':1, 'male': 0, 'female': 1})\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = {}\n",
    "# we will take cluster number from 1 to 50 and perform clustering while noting down the standard squared error\n",
    "for k in tqdm(range(1, 21)):\n",
    "    kmeans_elbow = KMeans(n_clusters=k, verbose=False).fit(t)\n",
    "    # inertia is sum of distances of samples to their closest cluster center\n",
    "    sse[k] = kmeans_elbow.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the SSE\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(list(sse.keys()), list(sse.values()), color='gray')\n",
    "\n",
    "# plotting the guidelines to find the optimum number of clusters\n",
    "ax.axhline(y=93000000, color='r', linestyle=\"dotted\")\n",
    "ax.axvline(x=4, color='r', linestyle=\"dotted\")\n",
    "\n",
    "# setting the plot labels and title\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "\n",
    "ax.set_xticks(range(21))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data (recommended for clustering)\n",
    "me = MinMaxScaler()\n",
    "t_s = me.fit_transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = {}\n",
    "# we will take cluster number from 1 to 50 and perform clustering while noting down the standard squared error\n",
    "for k in tqdm(range(1, 21)):\n",
    "    kmeans_elbow = KMeans(n_clusters=k, verbose=False).fit(t_s)\n",
    "    # inertia is sum of distances of samples to their closest cluster center\n",
    "    sse[k] = kmeans_elbow.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the SSE\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(list(sse.keys()), list(sse.values()), color='gray')\n",
    "\n",
    "# plotting the guidelines to find the optimum number of clusters\n",
    "ax.axhline(y=1200, color='r', linestyle=\"dotted\")\n",
    "ax.axvline(x=5, color='r', linestyle=\"dotted\")\n",
    "\n",
    "# setting the plot labels and title\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "\n",
    "ax.set_xticks(range(21))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figures, we can see that the unscaled clustering had a lot of SSE in compared to the scaled one. We proceed with that one and use cluster as 5 because this is a good number after which the SSE almost converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['cluster'] = KMeans(n_clusters=5, verbose=False, random_state=412).fit_predict(t_s)\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = t.groupby('cluster').revenue.sum().plot(kind='bar', figsize=(9, 5), color='gray')\n",
    "ax.set_ylabel('sum of revenue')\n",
    "ax.set_title('Distribution of Revenue across Clusters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = t.groupby('cluster').revenue.mean().plot(kind='bar', figsize=(9, 5), color='gray')\n",
    "\n",
    "ax.set_ylim([100, 145])\n",
    "\n",
    "ax.set_ylabel('mean of revenue')\n",
    "ax.set_title('Distribution of Quality of Revenue across Clusters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 0** has the users who have both best quality of revenue and the highest Revenue. <br>\n",
    "**Cluster 4** has the users who have both worst quality of revenue and the lowest Revenue. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(\n",
    "    list(\n",
    "        zip(\n",
    "            list((t[t.cluster==0].is_first_order.value_counts()/t.is_first_order.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==1].is_first_order.value_counts()/t.is_first_order.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==2].is_first_order.value_counts()/t.is_first_order.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==3].is_first_order.value_counts()/t.is_first_order.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==4].is_first_order.value_counts()/t.is_first_order.value_counts()).fillna(0).values)\n",
    "        )\n",
    "    )\n",
    ").T.plot(kind='bar', figsize=(9, 5), color=['gray', 'black'], alpha=0.6)\n",
    "\n",
    "l=plt.legend()\n",
    "l.get_texts()[0].set_text('Old')\n",
    "l.get_texts()[1].set_text('New')\n",
    "\n",
    "ax.set_ylabel('% users')\n",
    "ax.set_xlabel('cluster')\n",
    "ax.set_title('Distribution of Old and New users across Clusters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 0** has the users who are not first time buyers. <br>\n",
    "**Cluster 1 and 2** has the users who are first time buyers. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(\n",
    "    list(\n",
    "        zip(\n",
    "            list((t[t.cluster==0].country.value_counts()/t.country.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==1].country.value_counts()/t.country.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==2].country.value_counts()/t.country.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==3].country.value_counts()/t.country.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==4].country.value_counts()/t.country.value_counts()).fillna(0).values)\n",
    "        )\n",
    "    )\n",
    ").T.plot(kind='bar', figsize=(9, 5), color=['gray', 'black'], alpha=0.6)\n",
    "\n",
    "l=plt.legend()\n",
    "l.get_texts()[0].set_text('AU')\n",
    "l.get_texts()[1].set_text('NZ')\n",
    "\n",
    "ax.set_ylabel('% users')\n",
    "ax.set_xlabel('cluster')\n",
    "ax.set_title('Distribution of Users per Country across Clusters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 0** has the users who are mostly from AU. <br>\n",
    "**Cluster 4** has the users who are from NZ. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(\n",
    "    list(\n",
    "        zip(\n",
    "            list((t[t.cluster==0].user_gender.value_counts()/t.user_gender.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==1].user_gender.value_counts()/t.user_gender.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==2].user_gender.value_counts()/t.user_gender.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==3].user_gender.value_counts()/t.user_gender.value_counts()).fillna(0).values),\n",
    "            list((t[t.cluster==4].user_gender.value_counts()/t.user_gender.value_counts()).fillna(0).values)\n",
    "        )\n",
    "    )\n",
    ").T.plot(kind='bar', figsize=(9, 5), color=['gray', 'black'], alpha=0.6)\n",
    "\n",
    "l=plt.legend()\n",
    "l.get_texts()[0].set_text('male')\n",
    "l.get_texts()[1].set_text('female')\n",
    "\n",
    "ax.set_ylabel('% users')\n",
    "ax.set_xlabel('cluster')\n",
    "ax.set_title('Distribution of Users per Country across Clusters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 0, 2** has the users who are male. <br>\n",
    "**Cluster 1 and 3** has the users who are female. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame([\n",
    "    t[t.cluster==0].total_products.mean(),\n",
    "    t[t.cluster==1].total_products.mean(),\n",
    "    t[t.cluster==2].total_products.mean(),\n",
    "    t[t.cluster==3].total_products.mean(),\n",
    "    t[t.cluster==4].total_products.mean()\n",
    "]).plot(kind='bar', figsize=(9, 5), color=['gray', 'black'])\n",
    "\n",
    "ax.set_ylim([1.6, 2.3])\n",
    "\n",
    "ax.set_ylabel('mean basket size')\n",
    "ax.set_xlabel('cluster')\n",
    "ax.set_title('Mean Basket Size across Clusters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 0 and 3** has the users who have a considerable big basket size. <br>\n",
    "**Cluster 4** has the users who does not have a considerable big basket size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cluster 0**: has the **Australian, male, old-users** who have produce **best quality** and the **highest revenue**.<br>\n",
    "**cluster 1**: has the **first-time, female buyers**.<br>\n",
    "**cluster 2**: has the **first-time, male buyers.**<br>\n",
    "**cluster 3**: has the **old female users** who have a **big basket size**.<br>\n",
    "**cluster 4**: has the **New Zealand users** who produce **least quality revenue** and **neither have a big basket size**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
